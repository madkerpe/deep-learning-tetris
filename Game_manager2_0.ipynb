{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Game_manager2_0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-0YquV3T1B5w"
      },
      "source": [
        "# **Required imports**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iirQyndWXH2w",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "SRC_PATH = '/content/gdrive/My Drive/Colab Notebooks/TetrisEngine' # CHANGE ACCORDINGLY\n",
        "#SRC_PATH = '/content/gdrive/My Drive/Colab Notebooks/DL2020/Final project'\n",
        "#SRC_PATH = './'\n",
        "sys.path.append(SRC_PATH)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import time\n",
        "import cv2\n",
        "from time import sleep\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from collections import deque\n",
        "import pickle\n",
        "\n",
        "import itertools as it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g4mlXuOWQtq6"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ihewp3U-Qvss",
        "outputId": "9a1bda07-4b6b-4d4c-bb75-b5bec31eeea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zdqo2LV8iwm_"
      },
      "source": [
        "# **Importing the Tetris env**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OhZYkJJ60_fp",
        "colab": {}
      },
      "source": [
        "from engine import TetrisEngine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k3C-Uh2dABn2"
      },
      "source": [
        "# **Setting some parameters**\n",
        "This way we get rid of these \"magic numbers\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EpesmtZ2AK_H",
        "colab": {}
      },
      "source": [
        "ROTATE_LEFT_ACTION = 0\n",
        "ROTATE_RIGHT_ACTION = 1\n",
        "RIGHT_ACTION = 2\n",
        "LEFT_ACTION = 3\n",
        "SOFT_DROP_ACTION = 4\n",
        "HARD_DROP_ACTION = 5\n",
        "\n",
        "AGENT_OBSERVATION_SPACE = (20,10,1)\n",
        "AGENT_ACTION_SPACE = [ROTATE_LEFT_ACTION,ROTATE_RIGHT_ACTION,RIGHT_ACTION,LEFT_ACTION,SOFT_DROP_ACTION, HARD_DROP_ACTION]\n",
        "AGENT_ACTION_SPACE_REDUCED = [ROTATE_LEFT_ACTION,RIGHT_ACTION,LEFT_ACTION, HARD_DROP_ACTION]\n",
        "\n",
        "def INPUT_MAP(inp):\n",
        "  # 0 -> 0\n",
        "  # 1 -> 2\n",
        "  # 2 -> 3\n",
        "  # 3 -> 5\n",
        "\n",
        "  if (inp == 0):\n",
        "    return 0\n",
        "  if (inp == 1):\n",
        "    return 2\n",
        "  if (inp == 2):\n",
        "    return 3\n",
        "  if (inp == 3):\n",
        "    return 5\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jk5Rj-zq2KLy"
      },
      "source": [
        "# **Defining the agent (includes the training)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QmZc9taD2RdE",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, epsilon, epsilon_min, epsilon_decay, decay_change, epsilon_decay2):\n",
        "    self.action_size = len(AGENT_ACTION_SPACE_REDUCED)\n",
        "    self.observation_shape = AGENT_OBSERVATION_SPACE[0]*AGENT_OBSERVATION_SPACE[1]*AGENT_OBSERVATION_SPACE[2]\n",
        "\n",
        "    self.memory  = deque()\n",
        "    #self.reward_history = []\n",
        "    \n",
        "    self.gamma = 0.8\n",
        "\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_min = epsilon_min\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.decay_change = decay_change\n",
        "    self.epsilon_decay2 = epsilon_decay2\n",
        "\n",
        "    self.learning_rate = 0.001\n",
        "    self.tau = .05\n",
        "    self.errors = []\n",
        "    self.error_window_size = 100\n",
        "    self.epochs = 1\n",
        "    self.steps_since_target_update = 0\n",
        "    self.steps_between_target_updates = 100\n",
        "    self.replay_batch_size = 32\n",
        "    self.steps_between_training = 32\n",
        "    self.steps = 0\n",
        "\n",
        "    # does the actual predictions on what action to take\n",
        "    self.model = self.create_model()\n",
        "\n",
        "    # tracks what action we want our model to take.\n",
        "    # this network changes more slowly and tracks our eventual goal\n",
        "    self.target_model = self.create_model()\n",
        "\n",
        "  def log(self):\n",
        "    loss = np.mean(self.errors)\n",
        "    print(\"Epsilon=\"+str(self.epsilon)+\n",
        "          \". Avg q-value prediction NRMSE=\"+str(loss))\n",
        "    return self.epsilon, loss\n",
        "        \n",
        "  def get_action(self, observation):\n",
        "    if(self.epsilon>self.decay_change):\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "    else:\n",
        "        self.epsilon*=self.epsilon_decay2\n",
        "    self.epsilon = max(self.epsilon_min, self.epsilon)\n",
        "    \n",
        "    if(len(self.memory) < self.replay_batch_size or np.random.random() < self.epsilon):\n",
        "      return random.choice(range(self.action_size))\n",
        "\n",
        "    return np.argmax(self.model.predict(observation)[0])\n",
        "\n",
        "  def remember(self, observation, action, reward, new_observation, done):\n",
        "    self.memory.append([observation, action, reward, new_observation, done])\n",
        "    \"\"\"\n",
        "    self.reward_history.append(np.abs(reward))\n",
        "    if(len(self.reward_history) > self.avg_window_size):\n",
        "      self.reward_history = self.reward_history[:-self.avg_window_size]\n",
        "    \"\"\"\n",
        "\n",
        "  def create_model(self):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (5, 5), padding='same',input_shape=(20, 10, 1)))\n",
        "    model.add(Conv2D(64, (3,3), padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Conv2D(64, (3,3), padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation=\"relu\"))\n",
        "    model.add(Dense(256, activation=\"relu\"))\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "    #model.add(Dense(64, activation=\"relu\"))\n",
        "    model.add(Dense(self.action_size))\n",
        "\n",
        "    model.compile(loss=\"mean_squared_error\",\n",
        "    optimizer=Adam(lr=self.learning_rate))\n",
        "\n",
        "    return model\n",
        "\n",
        "  def __save_model_weights(self, ep, filename):\n",
        "    model_weights_path = SRC_PATH+\"/\"+filename+\"_\"+str(ep+1)+\".h5f\"\n",
        "    target_model_weights_path = SRC_PATH+\"/target_\"+filename+\"_\"+str(ep+1)+\".h5f\"\n",
        "    try:\n",
        "      self.model.save_weights(model_weights_path, overwrite=True)\n",
        "      self.target_model.save_weights(target_model_weights_path, overwrite=True)\n",
        "      print(\"Model weights saved succesfully\")\n",
        "    except:\n",
        "      print(\"Saving model weights failed. Continuing...\")\n",
        "\n",
        "  def __save_replay_buffer(self, ep, filename):\n",
        "    buffer_path = SRC_PATH+\"/\"+filename+\".pkl\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "      pickle.dump(self.memory, open(buffer_path, 'wb'))\n",
        "      print(\"Replay buffer saved succesfully\")\n",
        "    except:\n",
        "      print(\"Saving replay buffer failed. Continuing...\")\n",
        "    \"\"\"\n",
        "\n",
        "  def save_agent_state(self, ep, model_filename, buffer_filename):\n",
        "    self.__save_model_weights(ep, model_filename)\n",
        "    self.__save_replay_buffer(ep, buffer_filename)\n",
        "\n",
        "  def replay(self):\n",
        "    \n",
        "    if (len(self.memory) < self.replay_batch_size or self.steps < self.steps_between_training): \n",
        "        self.steps+=1\n",
        "        return\n",
        "    \n",
        "    self.steps=0\n",
        "    \n",
        "    for i in range(self.steps_between_training):\n",
        "        batch = random.sample(self.memory, self.replay_batch_size)\n",
        "        observations = np.array([x[0] for x in batch]).reshape(self.replay_batch_size, 20, 10, 1)\n",
        "        new_observations = np.array([x[3] for x in batch]).reshape(self.replay_batch_size, 20, 10, 1)\n",
        "        online_qs = self.model.predict(observations)\n",
        "        target_qs = self.target_model.predict(new_observations)\n",
        "\n",
        "        for i,sample in enumerate(batch):\n",
        "             _, action, reward, _, done = sample\n",
        "             if done:\n",
        "                 online_qs[i][action] = reward\n",
        "             else:\n",
        "                 q_next = np.max(target_qs[i])\n",
        "                 online_qs[i][action] = reward + q_next * self.gamma\n",
        "        e = self.model.fit(observations, online_qs, epochs=self.epochs, verbose=0)\n",
        "        self.errors.append(np.sqrt(np.mean(e.history[\"loss\"]))/np.abs(np.mean(online_qs)))\n",
        "        if(len(self.errors) > self.error_window_size):\n",
        "            self.errors = self.errors[-self.error_window_size:]\n",
        "\n",
        "  def target_train(self):\n",
        "    self.steps_since_target_update += 1\n",
        "    if self.steps_since_target_update >= self.steps_between_target_updates:\n",
        "      weights = self.model.get_weights()\n",
        "      self.target_model.set_weights(weights)\n",
        "      self.steps_since_target_update = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ikv1O_YJn1RZ"
      },
      "source": [
        "# **Filters the memories that are used for training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OYE_Kp_istw",
        "colab": {}
      },
      "source": [
        "class MemoryFilter():\n",
        "  def __init__(self, agent):\n",
        "    self.agent = agent\n",
        "    self.last_consulted_memory_item = None\n",
        "    self.last_unconsulted_memory_item = None\n",
        "    self.did_last_step_consult_agent = True\n",
        "\n",
        "  def remember(self, do_consult_agent, memory_item): \n",
        "    # (1)\n",
        "    # If we were not consulting the agent in the last step, but now we are (A new piece has been placed on top)\n",
        "    # -> Remember joined memories of last_consulted_memory_item and last_unconsulted_memory_item\n",
        "    if not self.did_last_step_consult_agent and do_consult_agent:\n",
        "      self.agent.remember(*joinMemories(self.last_consulted_memory_item, self.last_unconsulted_memory_item))            \n",
        "      self.agent.replay()       # internally iterates default (prediction) model\n",
        "      self.agent.target_train() # iterates target model\n",
        "\n",
        "      self.last_consulted_memory_item = None\n",
        "      self.last_unconsulted_memory_item = None\n",
        "\n",
        "    # (2)\n",
        "    # Do certain checks on item\n",
        "    # eg. nparray filled with zeros will be discarded\n",
        "    # eg. discard memory where no action is taken\n",
        "    valid_memory_item = True\n",
        "\n",
        "    # Stop if invalid\n",
        "    if not valid_memory_item:\n",
        "      return;\n",
        "\n",
        "    # (3)\n",
        "    if do_consult_agent:\n",
        "      self.agent.remember(*memory_item)            \n",
        "      self.agent.replay()       # internally iterates default (prediction) model\n",
        "      self.agent.target_train() # iterates target model\n",
        "      # save last memory-item in which the agent was consulted\n",
        "      self.last_consulted_memory_item = memory_item\n",
        "\n",
        "    else:\n",
        "      self.last_unconsulted_memory_item = memory_item\n",
        "\n",
        "  def joinMemories(memoryA, memoryB):\n",
        "    return [memoryA[3], memoryB[1], memoryB[2], memoryB[3], memoryB[4]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TQ6FbRGbtI6y"
      },
      "source": [
        "# **Reward function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fjPJ3fO_tN_e",
        "colab": {}
      },
      "source": [
        "# REMARK: The new engine's state also includes 2's (for the current block) besides 0's and 1's\n",
        "\n",
        "import engine\n",
        "import copy\n",
        "\n",
        "# Reward based on the difference in y-coordinates of the centroid of the subsequent states\n",
        "def custom_reward5(previous_state, current_state):\n",
        "    CENTROID_REWARD_FACTOR = 10\n",
        "    WIDTH_REWARD_FACTOR = 3\n",
        "    MAX_HEIGHT = 10\n",
        "    BLOCK_FACTOR = 1\n",
        "    stop = False\n",
        "    \n",
        "    h = engine.height(current_state)\n",
        "    if(h>MAX_HEIGHT):\n",
        "        stop = True\n",
        "    \n",
        "    previous_state = copy.deepcopy(previous_state)\n",
        "    previous_state[previous_state == 2] = 0\n",
        "    current_state = copy.deepcopy(current_state)\n",
        "    current_state[current_state == 2] = 0\n",
        "    \n",
        "    cy_previous = centroid_y(previous_state)\n",
        "    cy_current = centroid_y(current_state)\n",
        "    \n",
        "    pvs_w = max_width(previous_state)\n",
        "    c_w = max_width(current_state)\n",
        "    \n",
        "    current_block_height = engine.height(current_state)\n",
        "    previous_block_height = engine.height(previous_state)\n",
        "\n",
        "    centroid_reward = (cy_previous - cy_current)*CENTROID_REWARD_FACTOR\n",
        "    width_reward = max(c_w - pvs_w, 0)*WIDTH_REWARD_FACTOR\n",
        "    block_reward = (4 + previous_block_height - current_block_height)*BLOCK_FACTOR\n",
        "    \n",
        "    return centroid_reward + width_reward + block_reward, stop\n",
        "\n",
        "def centroid_y(state):\n",
        "    BOARD_HEIGHT = 20\n",
        "    hist = state.sum(1)\n",
        "    w = hist.sum()\n",
        "    if(w==0):\n",
        "        return 0\n",
        "    s = 0\n",
        "    for i,x in enumerate(hist):\n",
        "        s+=(BOARD_HEIGHT-1-i)*x\n",
        "    return s/w\n",
        "\n",
        "def custom_reward4(previous_state, current_state):\n",
        "  HEIGHT_REWARD_FACTOR = 1\n",
        "  WIDTH_REWARD_FACTOR = 1\n",
        "  MAX_HEIGHT = 10\n",
        "  stop = False\n",
        "\n",
        "  previous_state = copy.deepcopy(previous_state)\n",
        "  previous_state[previous_state == 2] = 0\n",
        "\n",
        "  current_state = copy.deepcopy(current_state)\n",
        "  current_state[current_state == 2] = 0\n",
        "\n",
        "  pvs_h = engine.height(previous_state)\n",
        "  c_h = engine.height(current_state)\n",
        "  if(c_h>MAX_HEIGHT):\n",
        "    stop = True\n",
        "\n",
        "  pvs_w = max_width(previous_state)\n",
        "  c_w = max_width(current_state)  \n",
        "\n",
        "  height_reward = (pvs_h - c_h)*HEIGHT_REWARD_FACTOR\n",
        "  width_reward = max(c_w - pvs_w,0)*WIDTH_REWARD_FACTOR\n",
        "\n",
        "  return height_reward + width_reward, stop\n",
        "\n",
        "def max_width(state):\n",
        "    LINES_TO_EXPLORE = 10\n",
        "    h = state.shape[0]\n",
        "    w = state.shape[1]\n",
        "    widths = []\n",
        "    for i in range(LINES_TO_EXPLORE):\n",
        "        widths.append(get_width(state[h-1-i]))\n",
        "    return max(widths)\n",
        "\n",
        "def get_width(a):\n",
        "    cnt = 0\n",
        "    res = 0\n",
        "    n = len(a)\n",
        "    for i in range(n):\n",
        "        if(a[i]==0):\n",
        "            cnt = 0\n",
        "        else:\n",
        "            cnt+=1\n",
        "            res = max(res,cnt)\n",
        "    return res\n",
        "\n",
        "def custom_reward3(state, steps, done, new_block):\n",
        "  MIN_STEPS_PER_EP = 1000\n",
        "  stop=False\n",
        "  h = engine.height(state)\n",
        "  w = max_width(state)\n",
        "  if(not(new_block)):\n",
        "    return 0, stop\n",
        "  else:\n",
        "    if(w<=5):\n",
        "      width_reward = -100*(10-w)\n",
        "    else:\n",
        "      width_reward = 100*w\n",
        "\n",
        "    if(h>=10):\n",
        "      steps_reward = steps - MIN_STEPS_PER_EP\n",
        "      height_reward = -1000*(h-5)\n",
        "      stop = True\n",
        "    elif(h<=4):\n",
        "      steps_reward = 0\n",
        "      height_reward = 1000*(5-h)\n",
        "    else:\n",
        "      steps_reward = 0\n",
        "      height_reward = 0\n",
        "    \n",
        "    return width_reward + height_reward + steps_reward, stop\n",
        "\n",
        "def custom_reward2(state):\n",
        "  try:\n",
        "    l = info['number_of_lines']\n",
        "  except:\n",
        "    l = 0\n",
        "  h = engine.height(state)\n",
        "  r = custom_reward(state)\n",
        "  if(h>=10):\n",
        "    return r-10000 + l*1000\n",
        "  elif(h<=4):\n",
        "    return r+10000 + l*1000\n",
        "  else:\n",
        "    return r + l*1000\n",
        "\n",
        "# reward parameters\n",
        "a = -0.510066\n",
        "b = 0.760666\n",
        "c = -0.35663\n",
        "d = -0.184483\n",
        "\n",
        "def custom_reward(new_observation):\n",
        "    new_observation = new_observation.reshape(20,10)\n",
        "\n",
        "    # ignore current piece area\n",
        "    for x in range(0, 5):\n",
        "        new_observation = np.delete(new_observation, 0, 0)\n",
        "\n",
        "    aggregate_height = compute_aggregate_height(new_observation)\n",
        "    complete_lines = compute_complete_lines(new_observation)\n",
        "    holes = compute_holes(new_observation)\n",
        "    bumpiness = compute_bumpiness(new_observation)\n",
        "    return a * aggregate_height + b * complete_lines + c * holes + d * bumpiness\n",
        "\n",
        "\n",
        "# REMARK: Similar functions exist in engine.py\n",
        "def compute_aggregate_height(observation):\n",
        "    aggregate_height = 0\n",
        "    for column in observation.T:\n",
        "        aggregate_height += compute_column_height(column)\n",
        "    return aggregate_height\n",
        "\n",
        "def compute_complete_lines(observation):\n",
        "    return  (observation.sum(axis=1) == 10).sum()\n",
        "\n",
        "\n",
        "def compute_holes(observation):\n",
        "    holes = 0\n",
        "    for column in observation.T:\n",
        "        prev_point = 0\n",
        "        for point in column:\n",
        "            if prev_point == 1 and point == 0:\n",
        "                holes += 1\n",
        "            prev_point = point \n",
        "    return holes\n",
        "\n",
        "def compute_bumpiness(observation):\n",
        "    bumpiness = 0\n",
        "    prev_height = None\n",
        "    for column in observation.T:\n",
        "        column_height = compute_column_height(column)\n",
        "        if prev_height != None:\n",
        "            bumpiness += abs(column_height - prev_height)\n",
        "        prev_height = column_height\n",
        "    return bumpiness\n",
        "\n",
        "# REMARK: Similar functions exist in engine.py\n",
        "def compute_column_height(column):\n",
        "    height = 0\n",
        "    found_top = False\n",
        "    for point in column:\n",
        "        if not found_top:\n",
        "            found_top = point == 1        \n",
        "        if found_top:\n",
        "            height += 1\n",
        "    return height"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YaiRfsRTvE2h"
      },
      "source": [
        "# Some code for plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UXShF_E05okA",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_plots(total_rewards, custom_rewards, epsilons, losses, scores, lines, steps):\n",
        "  MOVING_AVERAGE_N = 50\n",
        "\n",
        "  movav_total_rewards = np.convolve(total_rewards, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(total_rewards)), total_rewards)\n",
        "  plt.plot(np.arange(len(total_rewards) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_total_rewards, linewidth=2)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Reward\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  movav_custom_rewards = np.convolve(custom_rewards, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(custom_rewards)), custom_rewards)\n",
        "  plt.plot(np.arange(len(custom_rewards) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_custom_rewards, linewidth=2)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Reward\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(np.arange(len(epsilons)), epsilons)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Epsilon\")\n",
        "  plt.grid()\n",
        "  plt.ylim(0,1)\n",
        "  plt.show()\n",
        "\n",
        "  MOVING_AVERAGE_N = 10\n",
        "  movav_losses = np.convolve(losses, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(losses)), losses)\n",
        "  plt.plot(np.arange(len(losses) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_losses)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.grid()\n",
        "  plt.ylim(0,10)\n",
        "  plt.show()\n",
        "\n",
        "  MOVING_AVERAGE_N = 50\n",
        "\n",
        "  plt.plot(np.arange(len(scores)), scores)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Game score\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(np.arange(len(lines)), lines)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Number of cleared lines\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  movav_steps = np.convolve(steps, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(steps)), steps)\n",
        "  plt.plot(np.arange(len(steps) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_steps)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Number of steps\")\n",
        "  plt.grid()\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GwKUNqt7iSIa"
      },
      "source": [
        "# Trying some epsilon values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AC4CRn4eYbV3",
        "outputId": "5715dd30-681b-4b79-9de4-d70d95733674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#TODO: add plot for changing decay\n",
        "\n",
        "episodes = 10000\n",
        "mean_steps = 100\n",
        "\n",
        "epsilon = 0.9999\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.999995\n",
        "decay_change = 0.1\n",
        "epsilon_decay2 = 0.9999999\n",
        "\n",
        "epsilon_projection = np.arange(episodes*mean_steps)\n",
        "epsilon_projection = np.power(epsilon_decay, epsilon_projection)\n",
        "epsilon_projection[epsilon_projection < epsilon_min] = epsilon_min\n",
        "epsilon_projection = epsilon_projection[::mean_steps]\n",
        "\n",
        "plt.plot(range(len(epsilon_projection)), epsilon_projection)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Epsilon\")\n",
        "plt.grid()\n",
        "plt.ylim(0,1)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmxvUn2DTq0v"
      },
      "source": [
        "#**Setting up a game session (engine with hard drop)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-syhq27ST0S3",
        "outputId": "90550351-1c71-4e43-852f-9c319ca51f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import copy\n",
        "\n",
        "# initialising agent and game\n",
        "tetris_env = TetrisEngine()\n",
        "agent = Agent(epsilon, epsilon_min, epsilon_decay, decay_change, epsilon_decay2)\n",
        "memory_filter = MemoryFilter(agent)\n",
        "\n",
        "NUMBER_OF_EPISODES = 100\n",
        "champion_score = -10e6\n",
        "\n",
        "total_rewards = []\n",
        "custom_rewards = []\n",
        "epsilons = []\n",
        "losses = []\n",
        "scores = []\n",
        "lines = []\n",
        "steps = []\n",
        "\n",
        "# iterate over episodes\n",
        "print(\"Starting %d episodes: \" % NUMBER_OF_EPISODES)\n",
        "for ep in range(NUMBER_OF_EPISODES):\n",
        "\n",
        "  current_state = tetris_env.reset()\n",
        "  cond = (ep+1)%100==0\n",
        "  cur_step = 0\n",
        "  done = False\n",
        "  last_engine_reward = 0\n",
        "  tot_reward = 0\n",
        "  tot_custom_reward = 0\n",
        "  new_block = True\n",
        "\n",
        "  if(cond):\n",
        "    vid = cv2.VideoWriter(SRC_PATH+'/Gtetris_'+str(ep+1)+'.avi',cv2.VideoWriter_fourcc(*'XVID'), 15, (250, 500))\n",
        "    #vid = cv2.VideoWriter('tetris_'+str(ep+1)+'.avi',cv2.VideoWriter_fourcc(*'XVID'), 15, (250, 500))\n",
        "\n",
        "  # iterate over gamesteps\n",
        "  start = time.time()\n",
        "  while True:\n",
        "\n",
        "    cur_step+=1\n",
        "\n",
        "    # Render the game state\n",
        "    if(cond): #only save last episode to save processing time\n",
        "      frame = tetris_env.render()\n",
        "      vid.write(frame)\n",
        "\n",
        "    action = agent.get_action(current_state.reshape(1,20,10,1))\n",
        "    previous_state = current_state\n",
        "    current_state, last_engine_reward, done, info = tetris_env.step(INPUT_MAP(action))\n",
        "    new_block = info[\"new_block\"]\n",
        "      \n",
        "    # if there is a new piece\n",
        "    if (new_block or done):\n",
        "      our_custom_reward, done = custom_reward5(previous_state, current_state)\n",
        "      reward = our_custom_reward + last_engine_reward\n",
        "      tot_reward+=reward\n",
        "      tot_custom_reward+=our_custom_reward\n",
        "      memory_filter.remember(True, [previous_state, action, reward, current_state, done])\n",
        "\n",
        "      if (done):\n",
        "        end = time.time()\n",
        "        total_rewards.append(tot_reward)\n",
        "        custom_rewards.append(tot_custom_reward)\n",
        "        print(\"episode %d ended after: %d steps and %d sec., total reward is: %d \" % (ep, cur_step, end-start, tot_reward))\n",
        "        epsilon, loss = agent.log()\n",
        "        epsilons.append(epsilon)\n",
        "        losses.append(loss)\n",
        "        scores.append(info['score'])\n",
        "        lines.append(info['number_of_lines'])\n",
        "        steps.append(cur_step)\n",
        "\n",
        "        if (tot_reward > champion_score):\n",
        "          champion_score = tot_reward\n",
        "\n",
        "        if(cond):\n",
        "          vid.release()\n",
        "          print(previous_state)\n",
        "          make_plots(total_rewards, custom_rewards, epsilons, losses, scores, lines, steps)\n",
        "          agent.save_agent_state(ep, \"model_weights_G\", \"replay_buffer_G\")\n",
        "        \n",
        "        break\n",
        "\n",
        "    else:\n",
        "      memory_filter.remember(True, [previous_state, action, 0, current_state, done])   \n",
        "       \n",
        "print(\"The best score achieved equals %d\" % champion_score)\n",
        "make_plots(total_rewards, custom_rewards, epsilons, losses, scores, lines, steps)\n",
        "agent.save_agent_state(ep, \"model_weights_G\", \"replay_buffer_G\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting 100 episodes: \n",
            "episode 0 ended after: 16 steps and 0 sec., total reward is: -32 \n",
            "Epsilon=0.9998200109996297. Avg q-value prediction NRMSE=nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode 1 ended after: 24 steps and 9 sec., total reward is: -21 \n",
            "Epsilon=0.9997000394968142. Avg q-value prediction NRMSE=0.885314531783948\n",
            "episode 2 ended after: 27 steps and 2 sec., total reward is: -8 \n",
            "Epsilon=0.9995650887634837. Avg q-value prediction NRMSE=1.1850969708498043\n",
            "episode 3 ended after: 24 steps and 0 sec., total reward is: -28 \n",
            "Epsilon=0.9994451478495778. Avg q-value prediction NRMSE=1.1850969708498043\n",
            "episode 4 ended after: 15 steps and 2 sec., total reward is: -14 \n",
            "Epsilon=0.9993701920869752. Avg q-value prediction NRMSE=1.1727286598881264\n",
            "episode 5 ended after: 16 steps and 0 sec., total reward is: -31 \n",
            "Epsilon=0.9992902454696484. Avg q-value prediction NRMSE=1.1727286598881264\n",
            "episode 6 ended after: 30 steps and 2 sec., total reward is: -1 \n",
            "Epsilon=0.9991403627996012. Avg q-value prediction NRMSE=3.0861079942782084\n",
            "episode 7 ended after: 23 steps and 2 sec., total reward is: -21 \n",
            "Epsilon=0.99902546797722. Avg q-value prediction NRMSE=11.00588617203336\n",
            "episode 8 ended after: 27 steps and 2 sec., total reward is: -20 \n",
            "Epsilon=0.9988906083051257. Avg q-value prediction NRMSE=12.625573904696223\n",
            "episode 9 ended after: 13 steps and 0 sec., total reward is: -23 \n",
            "Epsilon=0.9988256823633861. Avg q-value prediction NRMSE=12.625573904696223\n",
            "episode 10 ended after: 18 steps and 2 sec., total reward is: -21 \n",
            "Epsilon=0.9987357918723793. Avg q-value prediction NRMSE=11.566654633389016\n",
            "episode 11 ended after: 34 steps and 2 sec., total reward is: -27 \n",
            "Epsilon=0.9985660207942826. Avg q-value prediction NRMSE=4.7587624002272575\n",
            "episode 12 ended after: 20 steps and 0 sec., total reward is: -13 \n",
            "Epsilon=0.9984661689352486. Avg q-value prediction NRMSE=4.7587624002272575\n",
            "episode 13 ended after: 30 steps and 2 sec., total reward is: -21 \n",
            "Epsilon=0.9983164098677203. Avg q-value prediction NRMSE=4.279137728336264\n",
            "episode 14 ended after: 29 steps and 2 sec., total reward is: -18 \n",
            "Epsilon=0.9981716641207442. Avg q-value prediction NRMSE=4.475871972568494\n",
            "episode 15 ended after: 20 steps and 2 sec., total reward is: -32 \n",
            "Epsilon=0.9980718516955046. Avg q-value prediction NRMSE=5.602526899065826\n",
            "episode 16 ended after: 39 steps and 2 sec., total reward is: -12 \n",
            "Epsilon=0.9978772461725638. Avg q-value prediction NRMSE=4.551626170738783\n",
            "episode 17 ended after: 11 steps and 0 sec., total reward is: -27 \n",
            "Epsilon=0.9978223642960845. Avg q-value prediction NRMSE=4.551626170738783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-babe67d7ca31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mtot_reward\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0mtot_custom_reward\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mour_custom_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m       \u001b[0mmemory_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprevious_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fbcf7b19c8a9>\u001b[0m in \u001b[0;36mremember\u001b[0;34m(self, do_consult_agent, memory_item)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_consult_agent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmemory_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# internally iterates default (prediction) model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# iterates target model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;31m# save last memory-item in which the agent was consulted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6edea40d0bc2>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m                  \u001b[0mq_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                  \u001b[0monline_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline_qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monline_qs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_window_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m   2580\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2581\u001b[0m         self._var_creator_scope.__exit__(\n\u001b[0;32m-> 2582\u001b[0;31m             exception_type, exception_value, traceback)\n\u001b[0m\u001b[1;32m   2583\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m         six.raise_from(\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}