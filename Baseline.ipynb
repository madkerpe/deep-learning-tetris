{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Game_manager_(1) (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-0YquV3T1B5w"
      },
      "source": [
        "# **Required imports**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iirQyndWXH2w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "644cab37-360e-42e1-fd50-8c91f70ac130"
      },
      "source": [
        "import sys\n",
        "SRC_PATH = '/content/gdrive/My Drive/Colab Notebooks/TetrisEngine' # CHANGE ACCORDINGLY\n",
        "#SRC_PATH = '/content/gdrive/My Drive/Colab Notebooks/DL2020/Final project'\n",
        "CHAMPION_PATH = '/content/gdrive/My Drive/Colab Notebooks/Champion'\n",
        "\n",
        "sys.path.append(SRC_PATH)\n",
        "sys.path.append(CHAMPION_PATH)\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "   %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import time\n",
        "import cv2\n",
        "from time import sleep\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from collections import deque\n",
        "import pickle\n",
        "\n",
        "import itertools as it"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g4mlXuOWQtq6"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ihewp3U-Qvss",
        "outputId": "5db5b61b-44b7-458b-b08a-819306fff0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zdqo2LV8iwm_"
      },
      "source": [
        "# **Importing the Tetris env**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OhZYkJJ60_fp",
        "colab": {}
      },
      "source": [
        "from engine import TetrisEngine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k3C-Uh2dABn2"
      },
      "source": [
        "# **Setting some parameters**\n",
        "This way we get rid of these \"magic numbers\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EpesmtZ2AK_H",
        "colab": {}
      },
      "source": [
        "ROTATE_LEFT_ACTION = 0\n",
        "ROTATE_RIGHT_ACTION = 1\n",
        "RIGHT_ACTION = 2\n",
        "LEFT_ACTION = 3\n",
        "SOFT_DROP_ACTION = 4\n",
        "HARD_DROP_ACTION = 5\n",
        "\n",
        "AGENT_OBSERVATION_SPACE = (20,10,1)\n",
        "AGENT_ACTION_SPACE = [ROTATE_LEFT_ACTION,ROTATE_RIGHT_ACTION,RIGHT_ACTION,LEFT_ACTION,SOFT_DROP_ACTION, HARD_DROP_ACTION]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jk5Rj-zq2KLy"
      },
      "source": [
        "# **Defining the agent (includes the training)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QmZc9taD2RdE",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self):\n",
        "    self.action_size = len(AGENT_ACTION_SPACE)\n",
        "    self.observation_shape = AGENT_OBSERVATION_SPACE[0]*AGENT_OBSERVATION_SPACE[1]*AGENT_OBSERVATION_SPACE[2]\n",
        "\n",
        "    self.memory  = deque()\n",
        "    #self.reward_history = []\n",
        "    \n",
        "    self.gamma = 0.8\n",
        "    self.epsilon = 0.9999\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.99999\n",
        "    self.learning_rate = 0.001\n",
        "    self.tau = .05\n",
        "    self.errors = []\n",
        "    self.epochs = 1\n",
        "    self.steps_since_target_update = 0\n",
        "    self.steps_between_target_updates = 100\n",
        "    self.replay_batch_size = 32\n",
        "    #self.avg_window_size = 32\n",
        "\n",
        "    # does the actual predictions on what action to take\n",
        "    self.model = self.create_model()\n",
        "\n",
        "    # tracks what action we want our model to take.\n",
        "    # this network changes more slowly and tracks our eventual goal\n",
        "    self.target_model = self.create_model()\n",
        "  \n",
        "  def new_episode(self):\n",
        "    self.errors = []\n",
        "\n",
        "  def log(self):\n",
        "    loss = np.mean(self.errors)\n",
        "    print(\"Epsilon=\"+str(self.epsilon)+\n",
        "          \". Avg q-value prediction NRMSE=\"+str(loss))\n",
        "    return self.epsilon, loss\n",
        "        \n",
        "  def get_action(self, observation):\n",
        "    self.epsilon *= self.epsilon_decay\n",
        "    self.epsilon = max(self.epsilon_min, self.epsilon)\n",
        "    \n",
        "    if(len(self.memory) < self.replay_batch_size or np.random.random() < self.epsilon):\n",
        "      return random.choice(range(self.action_size))\n",
        "\n",
        "    return np.argmax(self.model.predict(observation)[0])\n",
        "\n",
        "  def remember(self, observation, action, reward, new_observation, done):\n",
        "    self.memory.append([observation, action, reward, new_observation, done])\n",
        "    \"\"\"\n",
        "    self.reward_history.append(np.abs(reward))\n",
        "    if(len(self.reward_history) > self.avg_window_size):\n",
        "      self.reward_history = self.reward_history[:-self.avg_window_size]\n",
        "    \"\"\"\n",
        "\n",
        "  def create_model(self):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (5, 5), padding='same',input_shape=(20, 10, 1)))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3,3), padding='same'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    model.add(Conv2D(64, (3,3), padding='same'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=2))\n",
        "    #model.add(Conv2D(32, (3,3), padding='same'))\n",
        "    #model.add(BatchNormalization())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation=\"relu\"))\n",
        "    model.add(Dense(256, activation=\"relu\"))\n",
        "    #model.add(Dense(8, activation=\"relu\"))\n",
        "    model.add(Dense(self.action_size))\n",
        "\n",
        "    model.compile(loss=\"mean_squared_error\",\n",
        "    optimizer=Adam(lr=self.learning_rate))\n",
        "\n",
        "    model.load_weights(\"baseline-10000\")\n",
        "\n",
        "    return model\n",
        "\n",
        "  def __save_model_weights(self, ep):\n",
        "    model_weights_path = SRC_PATH+\"/model_weights_\"+str(ep)+\".h5f\"\n",
        "    target_model_weights_path = SRC_PATH+\"/target_model_weights_\"+str(ep)+\".h5f\"\n",
        "    try:\n",
        "      self.model.save_weights(model_weights_path, overwrite=True)\n",
        "      self.target_model.save_weights(target_model_weights_path, overwrite=True)\n",
        "      print(\"Model weights saved succesfully\")\n",
        "    except:\n",
        "      print(\"Saving model weights failed. Continuing...\")\n",
        "\n",
        "  def __save_replay_buffer(self, ep):\n",
        "    buffer_path = SRC_PATH+\"/replay_buffer_\"+str(ep)+\".pkl\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "      pickle.dump(self.memory, open(buffer_path, 'wb'))\n",
        "      print(\"Replay buffer saved succesfully\")\n",
        "    except:\n",
        "      print(\"Saving replay buffer failed. Continuing...\")\n",
        "    \"\"\"\n",
        "\n",
        "  def save_agent_state(self, ep):\n",
        "    self.__save_model_weights(ep)\n",
        "    self.__save_replay_buffer(ep)\n",
        "\n",
        "  def replay(self):\n",
        "    \n",
        "    if len(self.memory) < self.replay_batch_size: \n",
        "         return\n",
        "\n",
        "    batch = random.sample(self.memory, self.replay_batch_size)\n",
        "    observations = np.array([x[0] for x in batch]).reshape(self.replay_batch_size, 20, 10, 1)\n",
        "    new_observations = np.array([x[3] for x in batch]).reshape(self.replay_batch_size, 20, 10, 1)\n",
        "    online_qs = self.model.predict(observations)\n",
        "    target_qs = self.target_model.predict(new_observations)\n",
        "\n",
        "    for i,sample in enumerate(batch):\n",
        "         _, action, reward, _, done = sample\n",
        "         if done:\n",
        "             online_qs[i][action] = reward\n",
        "         else:\n",
        "             q_next = np.max(target_qs[i])\n",
        "             online_qs[i][action] = reward + q_next * self.gamma\n",
        "    e = self.model.fit(observations, online_qs, epochs=self.epochs, verbose=0)\n",
        "    self.errors.append(np.sqrt(np.mean(e.history[\"loss\"]))/np.abs(np.mean(online_qs)))\n",
        "\n",
        "  def target_train(self):\n",
        "    self.steps_since_target_update += 1\n",
        "    if self.steps_since_target_update >= self.steps_between_target_updates:\n",
        "      weights = self.model.get_weights()\n",
        "      self.target_model.set_weights(weights)\n",
        "      self.steps_since_target_update = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ikv1O_YJn1RZ"
      },
      "source": [
        "# **Filters the memories that are used for training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OYE_Kp_istw",
        "colab": {}
      },
      "source": [
        "class MemoryFilter():\n",
        "  def __init__(self, agent):\n",
        "    self.agent = agent\n",
        "    self.last_consulted_memory_item = None\n",
        "    self.last_unconsulted_memory_item = None\n",
        "    self.did_last_step_consult_agent = True\n",
        "\n",
        "    self.put_of_training = 0\n",
        "    self.epochs = 32\n",
        "\n",
        "  def remember(self, do_consult_agent, memory_item): \n",
        "    # (1)\n",
        "    # If we were not consulting the agent in the last step, but now we are (A new piece has been placed on top)\n",
        "    # -> Remember joined memories of last_consulted_memory_item and last_unconsulted_memory_item\n",
        "    if not self.did_last_step_consult_agent and do_consult_agent:\n",
        "      self.agent.remember(*joinMemories(self.last_consulted_memory_item, self.last_unconsulted_memory_item))            \n",
        "      self.agent.replay()       # internally iterates default (prediction) model\n",
        "      self.agent.target_train() # iterates target model\n",
        "\n",
        "      self.last_consulted_memory_item = None\n",
        "      self.last_unconsulted_memory_item = None\n",
        "\n",
        "    # (2)\n",
        "    # Do certain checks on item\n",
        "    # eg. nparray filled with zeros will be discarded\n",
        "    # eg. discard memory where no action is taken\n",
        "    valid_memory_item = True\n",
        "\n",
        "    # Stop if invalid\n",
        "    if not valid_memory_item:\n",
        "      return;\n",
        "\n",
        "    # (3)\n",
        "    if do_consult_agent:\n",
        "      self.agent.remember(*memory_item)\n",
        "      self.put_of_training += 1\n",
        "      if (self.put_of_training > self.epochs):\n",
        "        self.put_of_training = 0\n",
        "        self.agent.replay()       # internally iterates default (prediction) model\n",
        "        self.agent.target_train() # iterates target model\n",
        "      # save last memory-item in which the agent was consulted\n",
        "      self.last_consulted_memory_item = memory_item\n",
        "\n",
        "    else:\n",
        "      self.last_unconsulted_memory_item = memory_item\n",
        "\n",
        "  def joinMemories(memoryA, memoryB):\n",
        "    return [memoryA[3], memoryB[1], memoryB[2], memoryB[3], memoryB[4]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TQ6FbRGbtI6y"
      },
      "source": [
        "# **Reward function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fjPJ3fO_tN_e",
        "colab": {}
      },
      "source": [
        "# REMARK: The new engine's state also includes 2's (for the current block) besides 0's and 1's\n",
        "\n",
        "import engine\n",
        "\n",
        "def custom_reward4(previous_state, current_state):\n",
        "  MAX_HEIGHT = 10\n",
        "  HEIGHT_REWARD_FACTOR = 100\n",
        "  WIDTH_REWARD_FACTOR = 100\n",
        "  stop = False\n",
        "\n",
        "  pvs_h = engine.height(previous_state)\n",
        "  c_h = engine.height(current_state)\n",
        "  if(c_h>MAX_HEIGHT):\n",
        "    stop = True\n",
        "\n",
        "  pvs_w = max_width(previous_state)\n",
        "  c_w = max_width(current_state)  \n",
        "\n",
        "  height_reward = (pvs_h - c_h)*HEIGHT_REWARD_FACTOR\n",
        "  width_reward = max(c_w - pvs_w,0)*WIDTH_REWARD_FACTOR\n",
        "\n",
        "  return height_reward + width_reward, stop\n",
        "\n",
        "def max_width(state):\n",
        "    LINES_TO_EXPLORE = 4\n",
        "    h = state.shape[0]\n",
        "    w = state.shape[1]\n",
        "    widths = []\n",
        "    for i in range(LINES_TO_EXPLORE):\n",
        "        widths.append(get_width(state[h-1-i]))\n",
        "    return max(widths)\n",
        "\n",
        "def get_width(a):\n",
        "    cnt = 0\n",
        "    res = 0\n",
        "    n = len(a)\n",
        "    for i in range(n):\n",
        "        if(a[i]==0):\n",
        "            cnt = 0\n",
        "        else:\n",
        "            cnt+=1\n",
        "            res = max(res,cnt)\n",
        "    return res\n",
        "\n",
        "def custom_reward3(state, steps, done, new_block):\n",
        "  MIN_STEPS_PER_EP = 1000\n",
        "  stop=False\n",
        "  h = engine.height(state)\n",
        "  w = max_width(state)\n",
        "  if(not(new_block)):\n",
        "    return 0, stop\n",
        "  else:\n",
        "    if(w<=5):\n",
        "      width_reward = -100*(10-w)\n",
        "    else:\n",
        "      width_reward = 100*w\n",
        "\n",
        "    if(h>=10):\n",
        "      steps_reward = steps - MIN_STEPS_PER_EP\n",
        "      height_reward = -1000*(h-5)\n",
        "      stop = True\n",
        "    elif(h<=4):\n",
        "      steps_reward = 0\n",
        "      height_reward = 1000*(5-h)\n",
        "    else:\n",
        "      steps_reward = 0\n",
        "      height_reward = 0\n",
        "    \n",
        "    return width_reward + height_reward + steps_reward, stop\n",
        "\n",
        "def custom_reward2(state):\n",
        "  try:\n",
        "    l = info['number_of_lines']\n",
        "  except:\n",
        "    l = 0\n",
        "  h = engine.height(state)\n",
        "  r = custom_reward(state)\n",
        "  if(h>=10):\n",
        "    return r-10000 + l*1000\n",
        "  elif(h<=4):\n",
        "    return r+10000 + l*1000\n",
        "  else:\n",
        "    return r + l*1000\n",
        "\n",
        "# reward parameters\n",
        "a = -0.510066\n",
        "b = 0.760666\n",
        "c = -0.35663\n",
        "d = -0.184483\n",
        "\n",
        "def custom_reward(new_observation):\n",
        "    new_observation = new_observation.reshape(20,10)\n",
        "\n",
        "    # ignore current piece area\n",
        "    for x in range(0, 5):\n",
        "        new_observation = np.delete(new_observation, 0, 0)\n",
        "\n",
        "    aggregate_height = compute_aggregate_height(new_observation)\n",
        "    complete_lines = compute_complete_lines(new_observation)\n",
        "    holes = compute_holes(new_observation)\n",
        "    bumpiness = compute_bumpiness(new_observation)\n",
        "    return a * aggregate_height + b * complete_lines + c * holes + d * bumpiness\n",
        "\n",
        "\n",
        "# REMARK: Similar functions exist in engine.py\n",
        "def compute_aggregate_height(observation):\n",
        "    aggregate_height = 0\n",
        "    for column in observation.T:\n",
        "        aggregate_height += compute_column_height(column)\n",
        "    return aggregate_height\n",
        "\n",
        "def compute_complete_lines(observation):\n",
        "    return  (observation.sum(axis=1) == 10).sum()\n",
        "\n",
        "\n",
        "def compute_holes(observation):\n",
        "    holes = 0\n",
        "    for column in observation.T:\n",
        "        prev_point = 0\n",
        "        for point in column:\n",
        "            if prev_point == 1 and point == 0:\n",
        "                holes += 1\n",
        "            prev_point = point \n",
        "    return holes\n",
        "\n",
        "def compute_bumpiness(observation):\n",
        "    bumpiness = 0\n",
        "    prev_height = None\n",
        "    for column in observation.T:\n",
        "        column_height = compute_column_height(column)\n",
        "        if prev_height != None:\n",
        "            bumpiness += abs(column_height - prev_height)\n",
        "        prev_height = column_height\n",
        "    return bumpiness\n",
        "\n",
        "# REMARK: Similar functions exist in engine.py\n",
        "def compute_column_height(column):\n",
        "    height = 0\n",
        "    found_top = False\n",
        "    for point in column:\n",
        "        if not found_top:\n",
        "            found_top = point == 1        \n",
        "        if found_top:\n",
        "            height += 1\n",
        "    return height"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YaiRfsRTvE2h"
      },
      "source": [
        "# Some code for plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UXShF_E05okA",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_plots(rewards, epsilons, losses, scores, lines, steps):\n",
        "  MOVING_AVERAGE_N = 50\n",
        "\n",
        "  movav_rewards = np.convolve(rewards, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(rewards)), rewards)\n",
        "  plt.plot(np.arange(len(rewards) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_rewards, linewidth=2)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Reward\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  movav_epsilons = np.convolve(epsilons, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(epsilons)), epsilons)\n",
        "  plt.plot(np.arange(len(epsilons) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_epsilons)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Epsilon\")\n",
        "  plt.grid()\n",
        "  plt.ylim(0,1)\n",
        "  plt.show()\n",
        "\n",
        "  MOVING_AVERAGE_N = 10\n",
        "  movav_losses = np.convolve(losses, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(losses)), losses)\n",
        "  plt.plot(np.arange(len(losses) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_losses)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.grid()\n",
        "  plt.ylim(0,10)\n",
        "  plt.show()\n",
        "\n",
        "  MOVING_AVERAGE_N = 50\n",
        "  movav_scores = np.convolve(scores, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(scores)), scores)\n",
        "  plt.plot(np.arange(len(scores) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_scores)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Game score\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  movav_lines = np.convolve(lines, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(lines)), lines)\n",
        "  plt.plot(np.arange(len(lines) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_lines)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Number of cleared lines\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  movav_steps = np.convolve(steps, np.ones((MOVING_AVERAGE_N,))/MOVING_AVERAGE_N, mode='valid')\n",
        "  plt.plot(np.arange(len(steps)), steps)\n",
        "  plt.plot(np.arange(len(steps) - MOVING_AVERAGE_N + 1) + MOVING_AVERAGE_N - 1, movav_steps)\n",
        "  plt.xlabel(\"Episode\")\n",
        "  plt.ylabel(\"Number of steps\")\n",
        "  plt.grid()\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmxvUn2DTq0v",
        "colab_type": "text"
      },
      "source": [
        "#**Setting up a game session (engine with hard drop)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-syhq27ST0S3",
        "colab_type": "code",
        "outputId": "dc7e06ae-0265-49f8-9164-d1890eb75ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import copy\n",
        "\n",
        "# initialising agent and game\n",
        "tetris_env = TetrisEngine()\n",
        "agent = Agent()\n",
        "memory_filter = MemoryFilter(agent)\n",
        "\n",
        "NUMBER_OF_EPISODES = 20000\n",
        "champion_score = -10e6\n",
        "\n",
        "rewards = []\n",
        "epsilons = []\n",
        "losses = []\n",
        "scores = []\n",
        "lines = []\n",
        "steps = []\n",
        "\n",
        "# iterate over episodes\n",
        "print(\"Starting %d episodes: \" % NUMBER_OF_EPISODES)\n",
        "for ep in range(NUMBER_OF_EPISODES):\n",
        "\n",
        "  agent.new_episode()\n",
        "  current_state = tetris_env.reset()\n",
        "  cond = (ep+1)%250==0\n",
        "  cur_step = 0\n",
        "  done = False\n",
        "  last_engine_reward = 0\n",
        "  tot_reward = 0\n",
        "  new_block = True\n",
        "\n",
        "  if(cond):\n",
        "    vid = cv2.VideoWriter(SRC_PATH+'/Ctetris_'+str(ep+1)+'.avi',cv2.VideoWriter_fourcc(*'XVID'), 15, (250, 500))\n",
        "    #vid = cv2.VideoWriter('tetris_'+str(ep+1)+'.avi',cv2.VideoWriter_fourcc(*'XVID'), 15, (250, 500))\n",
        "\n",
        "  # iterate over gamesteps\n",
        "  start = time.time()\n",
        "  while True:\n",
        "\n",
        "    cur_step+=1\n",
        "\n",
        "    # Render the game state\n",
        "    if(cond): #only save last episode to save processing time\n",
        "      frame = tetris_env.render()\n",
        "      vid.write(frame)\n",
        "\n",
        "    action = agent.get_action(current_state.reshape(1,20,10,1))\n",
        "    previous_state = current_state\n",
        "    current_state, last_engine_reward, done, info = tetris_env.step(action)\n",
        "    new_block = info[\"new_block\"]\n",
        "      \n",
        "    # if there is a new piece\n",
        "    if (new_block or done):\n",
        "      our_custom_reward, done = custom_reward4(previous_state, current_state)\n",
        "      reward = our_custom_reward + 10*last_engine_reward\n",
        "      tot_reward+=reward\n",
        "      last_state_before_ground = copy.deepcopy(current_state)\n",
        "      last_state_before_ground[last_state_before_ground == 2] = 0\n",
        "      memory_filter.remember(True, [previous_state, action, reward, last_state_before_ground, done])\n",
        "\n",
        "      if (done):\n",
        "        end = time.time()\n",
        "        rewards.append(tot_reward)\n",
        "        print(\"episode %d ended after: %d steps and %d sec., total reward is: %d \" % (ep, cur_step, end-start, tot_reward))\n",
        "        epsilon, loss = agent.log()\n",
        "        epsilons.append(epsilon)\n",
        "        losses.append(loss)\n",
        "        scores.append(info['score'])\n",
        "        lines.append(info['number_of_lines'])\n",
        "        steps.append(cur_step)\n",
        "\n",
        "        if (tot_reward > champion_score):\n",
        "          champion_score = tot_reward\n",
        "\n",
        "        if(cond):\n",
        "          vid.release()\n",
        "          print(previous_state)\n",
        "          make_plots(rewards, epsilons, losses, scores, lines, steps)\n",
        "          agent.save_agent_state(ep)\n",
        "        \n",
        "        break\n",
        "\n",
        "    else:\n",
        "      memory_filter.remember(True, [previous_state, action, 0, current_state, done])   \n",
        "       \n",
        "print(\"The best score achieved equals %d\" % champion_score)\n",
        "make_plots(rewards, epsilons, losses, scores, lines, steps)\n",
        "agent.save_agent_state(ep)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for baseline-10000",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f9498dd3c386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# initialising agent and game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtetris_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTetrisEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmemory_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemoryFilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f798f8300b0d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# does the actual predictions on what action to take\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# tracks what action we want our model to take.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f798f8300b0d>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m     optimizer=Adam(lr=self.learning_rate))\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"baseline-10000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    249\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   def compile(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for baseline-10000"
          ]
        }
      ]
    }
  ]
}